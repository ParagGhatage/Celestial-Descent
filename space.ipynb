{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: pygame in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: Box2D in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: stable-baselines3 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (2.1.2)\n",
      "Requirement already satisfied: torch>=1.13 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (2.5.0)\n",
      "Requirement already satisfied: cloudpickle in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (3.1.0)\n",
      "Requirement already satisfied: pandas in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from stable-baselines3) (3.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: networkx in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in a:\\professional\\programming\\ml\\projects\\lander\\.venv\\lib\\site-packages (from opencv-python) (2.1.2)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/38.8 MB 13.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.4/38.8 MB 8.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.2/38.8 MB 7.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 5.0/38.8 MB 6.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.0/38.8 MB 5.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 7.6/38.8 MB 5.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/38.8 MB 6.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 10.5/38.8 MB 6.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.5/38.8 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.6/38.8 MB 5.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 13.9/38.8 MB 6.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 15.2/38.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 16.5/38.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.6/38.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 18.4/38.8 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.1/38.8 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.9/38.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.0/38.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/38.8 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 23.6/38.8 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 25.2/38.8 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.5/38.8 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/38.8 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/38.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.4/38.8 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.2/38.8 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.2/38.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.0/38.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.1/38.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.1/38.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.7/38.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "# ! pip install gym numpy pygame Box2D\n",
    "# ! pip install stable-baselines3\n",
    "# ! pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.8     |\n",
      "|    ep_rew_mean      | -125     |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 335      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.41     |\n",
      "|    n_updates        | 58       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.4     |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.928    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 35       |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 763      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 165      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.885    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 39       |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 1206     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.7      |\n",
      "|    n_updates        | 276      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 41       |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 1623     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.2      |\n",
      "|    n_updates        | 380      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -213     |\n",
      "|    exploration_rate | 0.808    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 42       |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 2025     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.44     |\n",
      "|    n_updates        | 481      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -204     |\n",
      "|    exploration_rate | 0.771    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 2407     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.35     |\n",
      "|    n_updates        | 576      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.718    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 2972     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.08     |\n",
      "|    n_updates        | 717      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 44       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 3558     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.679    |\n",
      "|    n_updates        | 864      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 44       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 4072     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.57     |\n",
      "|    n_updates        | 992      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 120      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 4797     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.927    |\n",
      "|    n_updates        | 1174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 120      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.496    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 5302     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 1300     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 5949     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 1462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 135      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 7026     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 1731     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 137      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.273    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 7653     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 1888     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 160      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.0885   |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total_timesteps  | 9595     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 2373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 174      |\n",
      "|    ep_rew_mean      | -191     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 239      |\n",
      "|    total_timesteps  | 11129    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.938    |\n",
      "|    n_updates        | 2757     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 199      |\n",
      "|    ep_rew_mean      | -191     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 13561    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.59     |\n",
      "|    n_updates        | 3365     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 320      |\n",
      "|    total_timesteps  | 15032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.706    |\n",
      "|    n_updates        | 3732     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 208      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 335      |\n",
      "|    total_timesteps  | 15776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.03     |\n",
      "|    n_updates        | 3918     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 211      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 358      |\n",
      "|    total_timesteps  | 16885    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.56     |\n",
      "|    n_updates        | 4196     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 212      |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 378      |\n",
      "|    total_timesteps  | 17836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 4433     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 390      |\n",
      "|    total_timesteps  | 18421    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.73     |\n",
      "|    n_updates        | 4580     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 210      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 408      |\n",
      "|    total_timesteps  | 19290    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.775    |\n",
      "|    n_updates        | 4797     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | -213     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 446      |\n",
      "|    total_timesteps  | 21135    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.834    |\n",
      "|    n_updates        | 5258     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 465      |\n",
      "|    total_timesteps  | 22035    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.603    |\n",
      "|    n_updates        | 5483     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 227      |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 487      |\n",
      "|    total_timesteps  | 23081    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.12     |\n",
      "|    n_updates        | 5745     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | -220     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 509      |\n",
      "|    total_timesteps  | 24174    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 6018     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 543      |\n",
      "|    total_timesteps  | 25809    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 6427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 282      |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 625      |\n",
      "|    total_timesteps  | 29809    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.45     |\n",
      "|    n_updates        | 7427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 302      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 676      |\n",
      "|    total_timesteps  | 32245    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.951    |\n",
      "|    n_updates        | 8036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 314      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 708      |\n",
      "|    total_timesteps  | 33797    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.22     |\n",
      "|    n_updates        | 8424     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 327      |\n",
      "|    ep_rew_mean      | -191     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 747      |\n",
      "|    total_timesteps  | 35691    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.779    |\n",
      "|    n_updates        | 8897     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 343      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 790      |\n",
      "|    total_timesteps  | 37813    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.85     |\n",
      "|    n_updates        | 9428     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 357      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 830      |\n",
      "|    total_timesteps  | 39727    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.788    |\n",
      "|    n_updates        | 9906     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 366      |\n",
      "|    ep_rew_mean      | -182     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 865      |\n",
      "|    total_timesteps  | 41441    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.7      |\n",
      "|    n_updates        | 10335    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 374      |\n",
      "|    ep_rew_mean      | -182     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 890      |\n",
      "|    total_timesteps  | 42665    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.715    |\n",
      "|    n_updates        | 10641    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 380      |\n",
      "|    ep_rew_mean      | -180     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 917      |\n",
      "|    total_timesteps  | 43959    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.713    |\n",
      "|    n_updates        | 10964    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 389      |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 957      |\n",
      "|    total_timesteps  | 45884    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 11445    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 981      |\n",
      "|    total_timesteps  | 47068    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.586    |\n",
      "|    n_updates        | 11741    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 410      |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 1053     |\n",
      "|    total_timesteps  | 50550    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.755    |\n",
      "|    n_updates        | 12612    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 412      |\n",
      "|    ep_rew_mean      | -148     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 1090     |\n",
      "|    total_timesteps  | 52342    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 13060    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 415      |\n",
      "|    ep_rew_mean      | -143     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 1145     |\n",
      "|    total_timesteps  | 55037    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 13734    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 426      |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 1198     |\n",
      "|    total_timesteps  | 57615    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.893    |\n",
      "|    n_updates        | 14378    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 449      |\n",
      "|    ep_rew_mean      | -120     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 1262     |\n",
      "|    total_timesteps  | 60702    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.876    |\n",
      "|    n_updates        | 15150    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 457      |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 1301     |\n",
      "|    total_timesteps  | 62574    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.968    |\n",
      "|    n_updates        | 15618    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlunarlander_dqn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32ma:\\PROFESSIONAL\\PROGRAMMING\\ML\\Projects\\Lander\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "# Create the model\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"lunarlander_dqn\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
